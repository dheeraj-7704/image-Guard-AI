{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5256696,"sourceType":"datasetVersion","datasetId":3041726}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries, load and transform data","metadata":{}},{"cell_type":"code","source":"# Install necessary Python packages using pip\n\n# Use the 'pip' command to install packages\n# The '-q' flag stands for 'quiet,' which means it will suppress most output, making the installation process less verbose\n# We're installing the following packages:\n# - 'evaluate': This package is likely used for evaluation purposes, but the specific functionality is not clear from this line alone\n# - 'transformers': This package is commonly used for natural language processing tasks, such as working with pre-trained language models like BERT or GPT\n# - 'datasets': This package provides easy access to various datasets commonly used in machine learning and natural language processing tasks\n# - 'mlflow': MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, including tracking experiments, packaging code into reproducible runs, and sharing and deploying models\n\n# Note: Before running this code, make sure you have Python and pip installed on your system.\n# Also, ensure you have an internet connection since pip will download and install these packages from PyPI (Python Package Index).\n!pip install -U -q evaluate transformers datasets>=2.14.5 accelerate>=0.27 mlflow 2>/dev/null","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:13:49.792490Z","iopub.execute_input":"2024-04-06T08:13:49.792872Z","iopub.status.idle":"2024-04-06T08:14:34.946041Z","shell.execute_reply.started":"2024-04-06T08:13:49.792839Z","shell.execute_reply":"2024-04-06T08:14:34.944974Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries and modules\nimport warnings  # Import the 'warnings' module for handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n\nimport gc  # Import the 'gc' module for garbage collection\nimport numpy as np  # Import NumPy for numerical operations\nimport pandas as pd  # Import Pandas for data manipulation\nimport itertools  # Import 'itertools' for iterators and looping\nfrom collections import Counter  # Import 'Counter' for counting elements\nimport matplotlib.pyplot as plt  # Import Matplotlib for data visualization\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\n# Import custom modules and classes\nfrom imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\nimport accelerate # Import the 'accelerate' module\nimport evaluate  # Import the 'evaluate' module\nfrom datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import (  # Import various modules from the Transformers library\n    TrainingArguments,  # For training arguments\n    Trainer,  # For model training\n    ViTImageProcessor,  # For processing image data with ViT models\n    ViTForImageClassification,  # ViT model for image classification\n    DefaultDataCollator  # For collating data in the default way\n)\nimport torch  # Import PyTorch for deep learning\nfrom torch.utils.data import DataLoader  # For creating data loaders\nfrom torchvision.transforms import (  # Import image transformation functions\n    CenterCrop,  # Center crop an image\n    Compose,  # Compose multiple image transformations\n    Normalize,  # Normalize image pixel values\n    RandomRotation,  # Apply random rotation to images\n    RandomResizedCrop,  # Crop and resize images randomly\n    RandomHorizontalFlip,  # Apply random horizontal flip\n    RandomAdjustSharpness,  # Adjust sharpness randomly\n    Resize,  # Resize images\n    ToTensor  # Convert images to PyTorch tensors\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:34.948150Z","iopub.execute_input":"2024-04-06T08:14:34.948444Z","iopub.status.idle":"2024-04-06T08:14:48.651449Z","shell.execute_reply.started":"2024-04-06T08:14:34.948410Z","shell.execute_reply":"2024-04-06T08:14:48.650593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import the necessary module from the Python Imaging Library (PIL).\nfrom PIL import ImageFile\n\n# Enable the option to load truncated images.\n# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:48.652515Z","iopub.execute_input":"2024-04-06T08:14:48.653467Z","iopub.status.idle":"2024-04-06T08:14:48.658291Z","shell.execute_reply.started":"2024-04-06T08:14:48.653436Z","shell.execute_reply":"2024-04-06T08:14:48.657361Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# use https://huggingface.co/docs/datasets/image_load for reference\n\n# Import necessary libraries\nimage_dict = {}\n\n# Define the list of file names\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport os\n# Initialize empty lists to store file names and labels\nfile_names = []\nlabels = []\n\n# Iterate through all image files in the specified directory\nfor file in sorted((Path('/kaggle/input/cifake-real-and-ai-generated-synthetic-images/').glob('*/*/*.*'))):\n    label = str(file).split('/')[-2]  # Extract the label from the file path\n    labels.append(label)  # Add the label to the list\n    file_names.append(str(file))  # Add the file path to the list\n\n# Print the total number of file names and labels\nprint(len(file_names), len(labels))\n\n# Create a pandas dataframe from the collected file names and labels\ndf = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:48.661017Z","iopub.execute_input":"2024-04-06T08:14:48.661319Z","iopub.status.idle":"2024-04-06T08:14:59.185403Z","shell.execute_reply.started":"2024-04-06T08:14:48.661294Z","shell.execute_reply":"2024-04-06T08:14:59.184505Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"120000 120000\n(120000, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:59.186367Z","iopub.execute_input":"2024-04-06T08:14:59.186700Z","iopub.status.idle":"2024-04-06T08:14:59.207514Z","shell.execute_reply.started":"2024-04-06T08:14:59.186675Z","shell.execute_reply":"2024-04-06T08:14:59.206133Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                               image label\n0  /kaggle/input/cifake-real-and-ai-generated-syn...  FAKE\n1  /kaggle/input/cifake-real-and-ai-generated-syn...  FAKE\n2  /kaggle/input/cifake-real-and-ai-generated-syn...  FAKE\n3  /kaggle/input/cifake-real-and-ai-generated-syn...  FAKE\n4  /kaggle/input/cifake-real-and-ai-generated-syn...  FAKE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/cifake-real-and-ai-generated-syn...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/cifake-real-and-ai-generated-syn...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/cifake-real-and-ai-generated-syn...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/cifake-real-and-ai-generated-syn...</td>\n      <td>FAKE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/cifake-real-and-ai-generated-syn...</td>\n      <td>FAKE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:59.208698Z","iopub.execute_input":"2024-04-06T08:14:59.208982Z","iopub.status.idle":"2024-04-06T08:14:59.445364Z","shell.execute_reply.started":"2024-04-06T08:14:59.208958Z","shell.execute_reply":"2024-04-06T08:14:59.444414Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['FAKE', 'REAL'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"# random oversampling of minority class\n# 'y' contains the target variable (label) we want to predict\ny = df[['label']]\n\n# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\ndf = df.drop(['label'], axis=1)\n\n# Create a RandomOverSampler object with a specified random seed (random_state=83)\nros = RandomOverSampler(random_state=83)\n\n# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\ndf, y_resampled = ros.fit_resample(df, y)\n\n# Delete the original 'y' variable to save memory as it's no longer needed\ndel y\n\n# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\ndf['label'] = y_resampled\n\n# Delete the 'y_resampled' variable to save memory as it's no longer needed\ndel y_resampled\n\n# Perform garbage collection to free up memory used by discarded variables\ngc.collect()\n\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:14:59.446526Z","iopub.execute_input":"2024-04-06T08:14:59.446910Z","iopub.status.idle":"2024-04-06T08:15:00.421366Z","shell.execute_reply.started":"2024-04-06T08:14:59.446873Z","shell.execute_reply":"2024-04-06T08:15:00.420374Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(120000, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a dataset from a Pandas DataFrame.\ndataset = Dataset.from_pandas(df).cast_column(\"image\", Image())","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:00.422396Z","iopub.execute_input":"2024-04-06T08:15:00.422665Z","iopub.status.idle":"2024-04-06T08:15:00.535015Z","shell.execute_reply.started":"2024-04-06T08:15:00.422642Z","shell.execute_reply":"2024-04-06T08:15:00.534033Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Display the first image in the dataset\ndataset[0][\"image\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:00.536126Z","iopub.execute_input":"2024-04-06T08:15:00.536390Z","iopub.status.idle":"2024-04-06T08:15:00.564061Z","shell.execute_reply.started":"2024-04-06T08:15:00.536366Z","shell.execute_reply":"2024-04-06T08:15:00.563192Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Extracting a subset of elements from the 'labels' list using slicing.\n# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n# This will give us the first 5 elements of the 'labels' list.\n# The result will be a new list containing these elements.\nlabels_subset = labels[:5]\n\n# Printing the subset of labels to inspect the content.\nprint(labels_subset)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:00.567885Z","iopub.execute_input":"2024-04-06T08:15:00.568203Z","iopub.status.idle":"2024-04-06T08:15:00.573504Z","shell.execute_reply.started":"2024-04-06T08:15:00.568175Z","shell.execute_reply":"2024-04-06T08:15:00.572547Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list of unique labels by converting 'labels' to a set and then back to a list\nlabels_list = ['REAL', 'FAKE'] #list(set(labels))\n\n# Initialize empty dictionaries to map labels to IDs and vice versa\nlabel2id, id2label = dict(), dict()\n\n# Iterate over the unique labels and assign each label an ID, and vice versa\nfor i, label in enumerate(labels_list):\n    label2id[label] = i  # Map the label to its corresponding ID\n    id2label[i] = label  # Map the ID to its corresponding label\n\n# Print the resulting dictionaries for reference\nprint(\"Mapping of IDs to Labels:\", id2label, '\\n')\nprint(\"Mapping of Labels to IDs:\", label2id)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:00.574818Z","iopub.execute_input":"2024-04-06T08:15:00.575156Z","iopub.status.idle":"2024-04-06T08:15:00.585457Z","shell.execute_reply.started":"2024-04-06T08:15:00.575123Z","shell.execute_reply":"2024-04-06T08:15:00.584502Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Mapping of IDs to Labels: {0: 'REAL', 1: 'FAKE'} \n\nMapping of Labels to IDs: {'REAL': 0, 'FAKE': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating classlabels to match labels to IDs\nClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n\n# Mapping labels to IDs\ndef map_label2id(example):\n    example['label'] = ClassLabels.str2int(example['label'])\n    return example\n\ndataset = dataset.map(map_label2id, batched=True)\n\n# Casting label column to ClassLabel Object\ndataset = dataset.cast_column('label', ClassLabels)\n\n# Splitting the dataset into training and testing sets using an 60-40 split ratio.\ndataset = dataset.train_test_split(test_size=0.4, shuffle=True, stratify_by_column=\"label\")\n\n# Extracting the training data from the split dataset.\ntrain_data = dataset['train']\n\n# Extracting the testing data from the split dataset.\ntest_data = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:00.586904Z","iopub.execute_input":"2024-04-06T08:15:00.587239Z","iopub.status.idle":"2024-04-06T08:15:01.149437Z","shell.execute_reply.started":"2024-04-06T08:15:00.587209Z","shell.execute_reply":"2024-04-06T08:15:01.148460Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c2076c0b9b4fd5964fc6cb3d75460f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4b2bb38cc74aaba3f5203451c287ae"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the pre-trained ViT model string\nmodel_str = \"dima806/ai_vs_real_image_detection\" #'google/vit-base-patch16-224-in21k'\n\n# Create a processor for ViT model input from the pre-trained model\nprocessor = ViTImageProcessor.from_pretrained(model_str)\n\n# Retrieve the image mean and standard deviation used for normalization\nimage_mean, image_std = processor.image_mean, processor.image_std\n\n# Get the size (height) of the ViT model's input images\nsize = processor.size[\"height\"]\nprint(\"Size: \", size)\n\n# Define a normalization transformation for the input images\nnormalize = Normalize(mean=image_mean, std=image_std)\n\n# Define a set of transformations for training data\n_train_transforms = Compose(\n    [\n        Resize((size, size)),             # Resize images to the ViT model's input size\n        RandomRotation(90),               # Apply random rotation\n        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n        ToTensor(),                       # Convert images to tensors\n        normalize                         # Normalize images using mean and std\n    ]\n)\n\n# Define a set of transformations for validation data\n_val_transforms = Compose(\n    [\n        Resize((size, size)),             # Resize images to the ViT model's input size\n        ToTensor(),                       # Convert images to tensors\n        normalize                         # Normalize images using mean and std\n    ]\n)\n\n# Define a function to apply training transformations to a batch of examples\ndef train_transforms(examples):\n    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples\n\n# Define a function to apply validation transformations to a batch of examples\ndef val_transforms(examples):\n    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n    return examples","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:01.151025Z","iopub.execute_input":"2024-04-06T08:15:01.151329Z","iopub.status.idle":"2024-04-06T08:15:01.423216Z","shell.execute_reply.started":"2024-04-06T08:15:01.151303Z","shell.execute_reply":"2024-04-06T08:15:01.422327Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/325 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd3c50b2715f457c94efd24538b362ff"}},"metadata":{}},{"name":"stdout","text":"Size:  224\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set the transforms for the training data\ntrain_data.set_transform(train_transforms)\n\n# Set the transforms for the test/validation data\ntest_data.set_transform(val_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:01.424387Z","iopub.execute_input":"2024-04-06T08:15:01.424685Z","iopub.status.idle":"2024-04-06T08:15:01.437094Z","shell.execute_reply.started":"2024-04-06T08:15:01.424659Z","shell.execute_reply":"2024-04-06T08:15:01.436150Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define a collate function that prepares batched data for model training.\ndef collate_fn(examples):\n    # Stack the pixel values from individual examples into a single tensor.\n    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n    \n    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n    labels = torch.tensor([example['label'] for example in examples])\n    \n    # Return a dictionary containing the batched pixel values and labels.\n    return {\"pixel_values\": pixel_values, \"labels\": labels}","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:01.438442Z","iopub.execute_input":"2024-04-06T08:15:01.438799Z","iopub.status.idle":"2024-04-06T08:15:01.454125Z","shell.execute_reply.started":"2024-04-06T08:15:01.438768Z","shell.execute_reply":"2024-04-06T08:15:01.453188Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Load, train, and evaluate model","metadata":{}},{"cell_type":"code","source":"# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\nmodel = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n\n# Configure the mapping of class labels to their corresponding indices for later reference.\nmodel.config.id2label = id2label\nmodel.config.label2id = label2id\n\n# Calculate and print the number of trainable parameters in millions for the model.\nprint(model.num_parameters(only_trainable=True) / 1e6)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:01.455349Z","iopub.execute_input":"2024-04-06T08:15:01.455705Z","iopub.status.idle":"2024-04-06T08:15:03.361383Z","shell.execute_reply.started":"2024-04-06T08:15:01.455673Z","shell.execute_reply":"2024-04-06T08:15:03.360380Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9e5c0b3ee974591a0227d195eda925c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4956640774c4157b06c1527161276b9"}},"metadata":{}},{"name":"stdout","text":"85.800194\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the accuracy metric from a module named 'evaluate'\naccuracy = evaluate.load(\"accuracy\")\n\n# Define a function 'compute_metrics' to calculate evaluation metrics\ndef compute_metrics(eval_pred):\n    # Extract model predictions from the evaluation prediction object\n    predictions = eval_pred.predictions\n    \n    # Extract true labels from the evaluation prediction object\n    label_ids = eval_pred.label_ids\n    \n    # Calculate accuracy using the loaded accuracy metric\n    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n    predicted_labels = predictions.argmax(axis=1)\n    \n    # Calculate accuracy score by comparing predicted labels to true labels\n    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n    \n    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n    return {\n        \"accuracy\": acc_score\n    }","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:03.362534Z","iopub.execute_input":"2024-04-06T08:15:03.362864Z","iopub.status.idle":"2024-04-06T08:15:04.087338Z","shell.execute_reply.started":"2024-04-06T08:15:03.362838Z","shell.execute_reply":"2024-04-06T08:15:04.086495Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"552d14891c004c72a871c98dc0625bdb"}},"metadata":{}}]},{"cell_type":"code","source":"# Define the name of the evaluation metric to be used during training and evaluation.\nmetric_name = \"accuracy\"\n\n# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\nmodel_name = \"ai_vs_real_image_detection\"\n\n# Define the number of training epochs for the model.\nnum_train_epochs = 2\n\n# Create an instance of TrainingArguments to configure training settings.\nargs = TrainingArguments(\n    # Specify the directory where model checkpoints and outputs will be saved.\n    output_dir=model_name,\n    \n    # Specify the directory where training logs will be stored.\n    logging_dir='./logs',\n    \n    # Define the evaluation strategy, which is performed at the end of each epoch.\n    evaluation_strategy=\"epoch\",\n    \n    # Set the learning rate for the optimizer.\n    learning_rate=1e-6,\n    \n    # Define the batch size for training on each device.\n    per_device_train_batch_size=64,\n    \n    # Define the batch size for evaluation on each device.\n    per_device_eval_batch_size=32,\n    \n    # Specify the total number of training epochs.\n    num_train_epochs=num_train_epochs,\n    \n    # Apply weight decay to prevent overfitting.\n    weight_decay=0.02,\n    \n    # Set the number of warm-up steps for the learning rate scheduler.\n    warmup_steps=50,\n    \n    # Disable the removal of unused columns from the dataset.\n    remove_unused_columns=False,\n    \n    # Define the strategy for saving model checkpoints (per epoch in this case).\n    save_strategy='epoch',\n    \n    # Load the best model at the end of training.\n    load_best_model_at_end=True,\n    \n    # Limit the total number of saved checkpoints to save space.\n    save_total_limit=1,\n    \n    # Specify that training progress should not be reported .\n    report_to=\"none\"  # log to none\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:04.088452Z","iopub.execute_input":"2024-04-06T08:15:04.088733Z","iopub.status.idle":"2024-04-06T08:15:04.173909Z","shell.execute_reply.started":"2024-04-06T08:15:04.088709Z","shell.execute_reply":"2024-04-06T08:15:04.173113Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Create a Trainer instance for fine-tuning a language model.\n\n# - `model`: The pre-trained language model to be fine-tuned.\n# - `args`: Configuration settings and hyperparameters for training.\n# - `train_dataset`: The dataset used for training the model.\n# - `eval_dataset`: The dataset used for evaluating the model during training.\n# - `data_collator`: A function that defines how data batches are collated and processed.\n# - `compute_metrics`: A function for computing custom evaluation metrics.\n# - `tokenizer`: The tokenizer used for processing text data.\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    data_collator=collate_fn,\n    compute_metrics=compute_metrics,\n    tokenizer=processor,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:04.175066Z","iopub.execute_input":"2024-04-06T08:15:04.175350Z","iopub.status.idle":"2024-04-06T08:15:04.421520Z","shell.execute_reply.started":"2024-04-06T08:15:04.175326Z","shell.execute_reply":"2024-04-06T08:15:04.420729Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Evaluate the pre-training model's performance on a test dataset.\n# This function calculates various metrics such as accuracy, loss, etc.,\n# to assess how well the model is performing on unseen data.\n\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:15:04.422541Z","iopub.execute_input":"2024-04-06T08:15:04.422828Z","iopub.status.idle":"2024-04-06T08:33:42.026866Z","shell.execute_reply.started":"2024-04-06T08:15:04.422803Z","shell.execute_reply":"2024-04-06T08:33:42.025937Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1500/1500 1:05:16]\n    </div>\n    "},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.04733540862798691,\n 'eval_accuracy': 0.9826666666666667,\n 'eval_runtime': 1117.5984,\n 'eval_samples_per_second': 42.949,\n 'eval_steps_per_second': 1.342}"},"metadata":{}}]},{"cell_type":"code","source":"# Start training the model using the trainer object.\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T08:33:42.028171Z","iopub.execute_input":"2024-04-06T08:33:42.028526Z","iopub.status.idle":"2024-04-06T09:57:10.926867Z","shell.execute_reply.started":"2024-04-06T08:33:42.028491Z","shell.execute_reply":"2024-04-06T09:57:10.925905Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2250/2250 1:23:25, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.036000</td>\n      <td>0.048707</td>\n      <td>0.982417</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.033400</td>\n      <td>0.048156</td>\n      <td>0.982500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2250, training_loss=0.03575980758666992, metrics={'train_runtime': 5008.5894, 'train_samples_per_second': 28.751, 'train_steps_per_second': 0.449, 'total_flos': 1.1158846504501248e+19, 'train_loss': 0.03575980758666992, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the post-training model's performance on the validation or test dataset.\n# This function computes various evaluation metrics like accuracy, loss, etc.\n# and provides insights into how well the model is performing.\n\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T09:57:10.928203Z","iopub.execute_input":"2024-04-06T09:57:10.928907Z","iopub.status.idle":"2024-04-06T10:08:19.263102Z","shell.execute_reply.started":"2024-04-06T09:57:10.928868Z","shell.execute_reply":"2024-04-06T10:08:19.262180Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.04815581813454628,\n 'eval_accuracy': 0.9825,\n 'eval_runtime': 668.3281,\n 'eval_samples_per_second': 71.821,\n 'eval_steps_per_second': 2.244,\n 'epoch': 2.0}"},"metadata":{}}]},{"cell_type":"code","source":"# Use the trained 'trainer' to make predictions on the 'test_data'.\noutputs = trainer.predict(test_data)\n\n# Print the metrics obtained from the prediction outputs.\nprint(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:08:19.264466Z","iopub.execute_input":"2024-04-06T10:08:19.264844Z","iopub.status.idle":"2024-04-06T10:19:36.010034Z","shell.execute_reply.started":"2024-04-06T10:08:19.264810Z","shell.execute_reply":"2024-04-06T10:19:36.009082Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"{'test_loss': 0.04815581813454628, 'test_accuracy': 0.9825, 'test_runtime': 676.7385, 'test_samples_per_second': 70.928, 'test_steps_per_second': 2.217}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extract the true labels from the model outputs\ny_true = outputs.label_ids\n\n# Predict the labels by selecting the class with the highest probability\ny_pred = outputs.predictions.argmax(1)\n\n# Define a function to plot a confusion matrix\ndef plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n    \"\"\"\n    This function plots a confusion matrix.\n\n    Parameters:\n        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n        title (str): Title for the plot.\n        cmap (matplotlib colormap): Colormap for the plot.\n    \"\"\"\n    # Create a figure with a specified size\n    plt.figure(figsize=figsize)\n    \n    # Display the confusion matrix as an image with a colormap\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    # Define tick marks and labels for the classes on the axes\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.0f'\n    # Add text annotations to the plot indicating the values in the cells\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Label the axes\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Ensure the plot layout is tight\n    plt.tight_layout()\n    # Display the plot\n    plt.show()\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average='macro')\n\n# Display accuracy and F1 score\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Get the confusion matrix if there are a small number of labels\nif len(labels_list) <= 150:\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Plot the confusion matrix using the defined function\n    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n    \n# Finally, display classification report\nprint()\nprint(\"Classification report:\")\nprint()\nprint(classification_report(y_true, y_pred, target_names=labels_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:36.011152Z","iopub.execute_input":"2024-04-06T10:19:36.011428Z","iopub.status.idle":"2024-04-06T10:19:36.447314Z","shell.execute_reply.started":"2024-04-06T10:19:36.011403Z","shell.execute_reply":"2024-04-06T10:19:36.446344Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Accuracy: 0.9825\nF1 Score: 0.9825\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\nClassification report:\n\n              precision    recall  f1-score   support\n\n        REAL     0.9868    0.9780    0.9824     24000\n        FAKE     0.9782    0.9870    0.9826     24000\n\n    accuracy                         0.9825     48000\n   macro avg     0.9825    0.9825    0.9825     48000\nweighted avg     0.9825    0.9825    0.9825     48000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model: This line of code is responsible for saving the model\n# that has been trained using the trainer object. It will serialize the model\n# and its associated weights, making it possible to reload and use the model\n# in the future without the need to retrain it.\ntrainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:36.448731Z","iopub.execute_input":"2024-04-06T10:19:36.449439Z","iopub.status.idle":"2024-04-06T10:19:37.113931Z","shell.execute_reply.started":"2024-04-06T10:19:36.449398Z","shell.execute_reply":"2024-04-06T10:19:37.113167Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Import the 'pipeline' function from the 'transformers' library.\nfrom transformers import pipeline\n\n# Create a pipeline for image classification tasks. \n# You need to specify the 'model_name' and the 'device' to use for inference.\n# - 'model_name': The name of the pre-trained model to be used for image classification.\n# - 'device': Specifies the device to use for running the model (0 for GPU, -1 for CPU).\npipe = pipeline('image-classification', model=model_name, device=0)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:37.115063Z","iopub.execute_input":"2024-04-06T10:19:37.115401Z","iopub.status.idle":"2024-04-06T10:19:37.414244Z","shell.execute_reply.started":"2024-04-06T10:19:37.115370Z","shell.execute_reply":"2024-04-06T10:19:37.413468Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Accessing an image from the 'test_data' dataset using index 1.\nimage = test_data[1][\"image\"]\n\n# Displaying the 'image' variable.\nimage","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:37.415226Z","iopub.execute_input":"2024-04-06T10:19:37.415489Z","iopub.status.idle":"2024-04-06T10:19:37.425097Z","shell.execute_reply.started":"2024-04-06T10:19:37.415466Z","shell.execute_reply":"2024-04-06T10:19:37.424281Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=32x32>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# Apply the 'pipe' function to process the 'image' variable.\npipe(image)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:37.429413Z","iopub.execute_input":"2024-04-06T10:19:37.429785Z","iopub.status.idle":"2024-04-06T10:19:37.464060Z","shell.execute_reply.started":"2024-04-06T10:19:37.429759Z","shell.execute_reply":"2024-04-06T10:19:37.463190Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"[{'label': 'REAL', 'score': 0.9806844592094421},\n {'label': 'FAKE', 'score': 0.019315555691719055}]"},"metadata":{}}]},{"cell_type":"code","source":"# This line of code accesses the \"label\" attribute of a specific element in the test_data list.\n# It's used to retrieve the actual label associated with a test data point.\nid2label[test_data[1][\"label\"]]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:19:37.465188Z","iopub.execute_input":"2024-04-06T10:19:37.465520Z","iopub.status.idle":"2024-04-06T10:19:37.473683Z","shell.execute_reply.started":"2024-04-06T10:19:37.465485Z","shell.execute_reply":"2024-04-06T10:19:37.472928Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'REAL'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Send model to Huggingface","metadata":{}},{"cell_type":"code","source":"# Import the necessary module to interact with the Hugging Face Hub.\nfrom huggingface_hub import notebook_login\n\n# Perform a login to the Hugging Face Hub.\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:21:31.594499Z","iopub.execute_input":"2024-04-06T10:21:31.594979Z","iopub.status.idle":"2024-04-06T10:21:31.620896Z","shell.execute_reply.started":"2024-04-06T10:21:31.594942Z","shell.execute_reply":"2024-04-06T10:21:31.620050Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf6fb3df5d8b4987b42453cb5e0636ca"}},"metadata":{}}]},{"cell_type":"code","source":"# Import the HfApi class from the huggingface_hub library.\nfrom huggingface_hub import HfApi\n\n# Create an instance of the HfApi class.\napi = HfApi()\n\n# Define the repository ID by combining the username \"dima806\" with the model name.\nrepo_id = f\"dima806/{model_name}\"\n\ntry:\n    # Attempt to create a new repository on the Hugging Face Model Hub using the specified repo_id.\n    api.create_repo(repo_id)\n    \n    # If the repository creation is successful, print a message indicating that the repository was created.\n    print(f\"Repo {repo_id} created\")\nexcept:\n    # If an exception is raised, print a message indicating that the repository already exists.\n    print(f\"Repo {repo_id} already exists\")","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:21:50.066521Z","iopub.execute_input":"2024-04-06T10:21:50.067246Z","iopub.status.idle":"2024-04-06T10:21:50.165731Z","shell.execute_reply.started":"2024-04-06T10:21:50.067209Z","shell.execute_reply":"2024-04-06T10:21:50.164876Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Repo dima806/ai_vs_real_image_detection already exists\n","output_type":"stream"}]},{"cell_type":"code","source":"# Uploading a folder to the Hugging Face Model Hub\napi.upload_folder(\n    folder_path=model_name,  # The path to the folder to be uploaded\n    path_in_repo=\".\",  # The path where the folder will be stored in the repository\n    repo_id=repo_id,  # The ID of the repository where the folder will be uploaded\n    repo_type=\"model\",  # The type of the repository (in this case, a model repository)\n    revision=\"main\" # Revision name\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T10:21:50.510138Z","iopub.execute_input":"2024-04-06T10:21:50.510492Z","iopub.status.idle":"2024-04-06T10:22:21.116607Z","shell.execute_reply.started":"2024-04-06T10:21:50.510461Z","shell.execute_reply":"2024-04-06T10:22:21.115686Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 7 LFS files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2743ae6265a44e3595186bf6724a277b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"401f896da792435299c44bbf59c744ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9761f05885384afebbd09db75a9b0729"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e56566bf5e7e4583b35e8e20959dd753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d1aa7886ac47ff8c93065e67d1da2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/687M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b6ed9fdf0d4a1ab8958cebbf8fc567"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974125b196a74eedbfd52ef9951cdd00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c2a98d3c724c4b9b090ca9249c412d"}},"metadata":{}},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/dima806/ai_vs_real_image_detection/commit/081e2ff015588985738a35655bd7ae7a56033ffb', commit_message='Upload folder using huggingface_hub', commit_description='', oid='081e2ff015588985738a35655bd7ae7a56033ffb', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}